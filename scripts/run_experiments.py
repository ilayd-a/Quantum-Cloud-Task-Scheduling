"""Experiment orchestration for QAOA vs classical schedulers."""

from __future__ import annotations

import argparse
import csv
import json
import sys
from argparse import Namespace
from datetime import datetime, UTC
from pathlib import Path
from typing import Any, Dict

import yaml

ROOT = Path(__file__).resolve().parents[1]
sys.path.append(str(ROOT / "src"))

from quantum_scheduler import solve_qaoa_local
from quantum_scheduler.classical_solver import solve_classical
from quantum_scheduler.utils import load_tasks


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument("--dataset", default="dataset_5.csv", help="Dataset filename or absolute path")
    parser.add_argument("--config", type=Path, default=None, help="Optional YAML config to run a batch of experiments")
    parser.add_argument("--reps", type=int, default=1, help="Number of QAOA layers")
    parser.add_argument("--maxiter", type=int, default=30, help="Maximum COBYLA iterations")
    parser.add_argument("--shots", type=int, default=1024, help="Shots per energy evaluation during optimization")
    parser.add_argument("--final-shots", type=int, default=4096, help="Shots for the post-optimization sampling pass")
    parser.add_argument("--machines", type=int, default=2, help="Number of machines for the classical baseline")
    parser.add_argument("--balance-penalty", type=float, default=None, help="Penalty applied to the load-balance term of the QUBO (default: 10 * total load)")
    parser.add_argument("--priority-bias", type=float, default=1.0, help="Linear bias encouraging high-priority tasks")
    parser.add_argument("--output", default=None, help="Optional JSON output path for a single run")
    parser.add_argument("--tag", default=None, help="Optional string appended to autogenerated filenames")
    return parser.parse_args()


def resolve_dataset_path(dataset_arg: str | Path) -> Path:
    dataset_path = Path(dataset_arg)
    if not dataset_path.is_absolute():
        dataset_path = ROOT / "data" / "datasets" / dataset_path
    if not dataset_path.exists():
        raise FileNotFoundError(f"Dataset not found: {dataset_path}")
    return dataset_path


def build_output_path(dataset_path: Path, args: argparse.Namespace) -> Path:
    if args.output:
        return Path(args.output)
    suffix = f"_r{args.reps}_shots{args.final_shots}"
    if args.tag:
        suffix += f"_{args.tag}"
    return ROOT / "results" / f"{dataset_path.stem}{suffix}.json"


def run_single_experiment(args: argparse.Namespace, dataset_override: str | Path | None = None, output_override: str | Path | None = None) -> Dict[str, Any]:
    dataset_path = resolve_dataset_path(dataset_override or args.dataset)
    tasks = load_tasks(dataset_path)

    qaoa_res = solve_qaoa_local(
        tasks,
        reps=args.reps,
        maxiter=args.maxiter,
        shots=args.shots,
        final_shots=args.final_shots,
        balance_penalty=args.balance_penalty,
        priority_bias=args.priority_bias,
    )
    classical_res = solve_classical([task["p_i"] for task in tasks], M=args.machines)

    qaoa_schedule = (qaoa_res.get("best_schedule") or {})
    qaoa_makespan = qaoa_schedule.get("makespan")
    classical_makespan = classical_res.get("makespan")

    relative_gap = None
    if qaoa_makespan is not None and classical_makespan:
        relative_gap = 100.0 * (qaoa_makespan - classical_makespan) / classical_makespan

    payload: Dict[str, Any] = {
        "dataset": dataset_path.name,
        "dataset_path": str(dataset_path),
        "num_tasks": len(tasks),
        "timestamp": datetime.now(UTC).isoformat(timespec="seconds"),
        "config": {
            "reps": args.reps,
            "maxiter": args.maxiter,
            "shots": args.shots,
            "final_shots": args.final_shots,
            "machines": args.machines,
            "balance_penalty": args.balance_penalty,
            "priority_bias": args.priority_bias,
        },
        "qaoa": qaoa_res,
        "classical": classical_res,
        "metrics": {
            "qaoa_makespan": qaoa_makespan,
            "classical_makespan": classical_makespan,
            "relative_gap_pct": relative_gap,
        },
    }

    output_path = Path(output_override) if output_override else build_output_path(dataset_path, args)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(json.dumps(payload, indent=2))
    payload["output_path"] = str(output_path)

    summary = payload["metrics"]
    print(
        f"Saved {output_path} | makespans (QAOA={summary['qaoa_makespan']}, classical={summary['classical_makespan']}), gap={summary['relative_gap_pct']}"
    )

    return payload


def run_from_config(config_path: Path, base_args: argparse.Namespace) -> None:
    config = yaml.safe_load(config_path.read_text()) or {}
    experiments = config.get("experiments", [])
    if not experiments:
        raise ValueError(f"No experiments defined in {config_path}")

    output_csv = Path(config.get("output_csv") or (ROOT / "results" / f"{config_path.stem}.csv"))
    rows = []

    for exp in experiments:
        overrides = vars(base_args).copy()
        overrides.update(exp)
        exp_args = Namespace(**overrides)
        payload = run_single_experiment(
            exp_args,
            dataset_override=exp.get("dataset"),
            output_override=exp.get("output"),
        )
        metrics = payload["metrics"]
        rows.append(
            {
                "dataset": payload["dataset"],
                "num_tasks": payload["num_tasks"],
                "reps": exp_args.reps,
                "shots": exp_args.shots,
                "final_shots": exp_args.final_shots,
                "balance_penalty": exp_args.balance_penalty,
                "priority_bias": exp_args.priority_bias,
                "qaoa_makespan": metrics.get("qaoa_makespan"),
                "classical_makespan": metrics.get("classical_makespan"),
                "relative_gap_pct": metrics.get("relative_gap_pct"),
                "output_path": payload["output_path"],
            }
        )

    output_csv.parent.mkdir(parents=True, exist_ok=True)
    with output_csv.open("w", newline="") as fh:
        writer = csv.DictWriter(
            fh,
            fieldnames=[
                "dataset",
                "num_tasks",
                "reps",
                "shots",
                "final_shots",
                "balance_penalty",
                "priority_bias",
                "qaoa_makespan",
                "classical_makespan",
                "relative_gap_pct",
                "output_path",
            ],
        )
        writer.writeheader()
        writer.writerows(rows)

    print(f"Batch summary saved to {output_csv}")


def main() -> None:
    args = parse_args()
    if args.config:
        run_from_config(args.config, args)
    else:
        run_single_experiment(args)


if __name__ == "__main__":
    main()
