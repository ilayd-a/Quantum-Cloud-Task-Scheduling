"""Experiment orchestration for QAOA vs classical schedulers."""

from __future__ import annotations

import argparse
import csv
import json
import sys
from argparse import Namespace
from datetime import datetime, UTC
from pathlib import Path
from typing import Any, Dict

import yaml

ROOT = Path(__file__).resolve().parents[1]
sys.path.append(str(ROOT / "src"))

from quantum_scheduler import solve_qaoa_local
from quantum_scheduler.classical_solver import solve_classical, solve_greedy, solve_ilp, solve_lpt
from quantum_scheduler.utils import load_tasks


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument("--dataset", default="dataset_5.csv", help="Dataset filename or absolute path")
    parser.add_argument("--config", type=Path, default=None, help="Optional YAML config to run a batch of experiments")
    parser.add_argument("--reps", type=int, default=1, help="Number of QAOA layers")
    parser.add_argument("--maxiter", type=int, default=30, help="Maximum COBYLA iterations")
    parser.add_argument("--shots", type=int, default=1024, help="Shots per energy evaluation during optimization")
    parser.add_argument("--final-shots", type=int, default=4096, help="Shots for the post-optimization sampling pass")
    parser.add_argument("--balance-penalty", type=float, default=10.0, help="Penalty multiplier A; actual penalty = value × total_load (default: 10)")
    parser.add_argument("--makespan-penalty", type=float, default=None, help="Makespan penalty multiplier C; directly penalizes high loads (default: 2× balance_penalty)")
    parser.add_argument("--priority-bias", type=float, default=0.1, help="Linear bias encouraging high-priority tasks on better machines")
    parser.add_argument("--optimizer", choices=["cobyla", "spsa"], default="cobyla", help="Classical optimizer for QAOA parameters")
    parser.add_argument("--restarts", type=int, default=5, help="Number of random restarts for the optimizer")
    parser.add_argument("--backend", choices=["aer", "statevector"], default="aer", help="Backend type for energy evaluation")
    parser.add_argument("--seed", type=int, default=None, help="Random seed for initial angles")
    parser.add_argument("--output", default=None, help="Optional JSON output path for a single run")
    parser.add_argument("--tag", default=None, help="Optional string appended to autogenerated filenames")
    parser.add_argument("--num-runs", type=int, default=1, help="Number of independent runs (for statistical analysis)")
    parser.add_argument("--classical-method", choices=["brute_force", "ilp", "greedy", "lpt"], default="brute_force", help="Classical solver method")
    return parser.parse_args()


def resolve_dataset_path(dataset_arg: str | Path) -> Path:
    dataset_path = Path(dataset_arg)
    if not dataset_path.is_absolute():
        dataset_path = ROOT / "data" / "datasets" / dataset_path
    if not dataset_path.exists():
        raise FileNotFoundError(f"Dataset not found: {dataset_path}")
    return dataset_path


def build_output_path(dataset_path: Path, args: argparse.Namespace, run_id: int | None = None) -> Path:
    if args.output:
        if run_id is not None and args.num_runs > 1:
            # Append run_id to output path for multiple runs
            output_path = Path(args.output)
            return output_path.parent / f"{output_path.stem}_run{run_id}{output_path.suffix}"
        return Path(args.output)
    suffix = f"_r{args.reps}_shots{args.final_shots}_results"
    if args.tag:
        suffix += f"_{args.tag}"
    if run_id is not None and args.num_runs > 1:
        suffix += f"_run{run_id}"
    return ROOT / "results" / f"{dataset_path.stem}{suffix}_results.json"


def run_single_experiment(args: argparse.Namespace, dataset_override: str | Path | None = None, output_override: str | Path | None = None, run_id: int | None = None) -> Dict[str, Any]:
    dataset_path = resolve_dataset_path(dataset_override or args.dataset)
    tasks = load_tasks(dataset_path)

    # Use different seed for each run if multiple runs requested
    experiment_seed = args.seed
    if run_id is not None and args.seed is not None:
        experiment_seed = args.seed + run_id

    qaoa_res = solve_qaoa_local(
        tasks,
        reps=args.reps,
        maxiter=args.maxiter,
        shots=args.shots,
        final_shots=args.final_shots,
        balance_penalty=args.balance_penalty,
        priority_bias=args.priority_bias,
        makespan_penalty=args.makespan_penalty,
        optimizer=args.optimizer,
        restarts=args.restarts,
        backend_type=args.backend,
        seed=experiment_seed,
    )
    
    # Choose classical solver method
    processing_times = [task["p_i"] for task in tasks]
    if args.classical_method == "ilp":
        try:
            classical_res = solve_ilp(processing_times, M=2)
        except Exception as e:
            print(f"Warning: ILP solver failed ({e}), falling back to brute force")
            classical_res = solve_classical(processing_times, M=2)
    elif args.classical_method == "greedy":
        classical_res = solve_greedy(processing_times, M=2)
    elif args.classical_method == "lpt":
        classical_res = solve_lpt(processing_times, M=2)
    else:
        classical_res = solve_classical(processing_times, M=2)

    qaoa_schedule = (qaoa_res.get("best_schedule") or {})
    qaoa_makespan = qaoa_schedule.get("makespan")
    min_makespan_in_samples = qaoa_res.get("min_makespan_in_samples")
    classical_makespan = classical_res.get("makespan")

    relative_gap = None
    if qaoa_makespan is not None and classical_makespan:
        relative_gap = 100.0 * (qaoa_makespan - classical_makespan) / classical_makespan
    
    # Also compute gap for best makespan found in samples (vs energy-selected)
    best_sample_gap = None
    if min_makespan_in_samples is not None and classical_makespan:
        best_sample_gap = 100.0 * (min_makespan_in_samples - classical_makespan) / classical_makespan

    payload: Dict[str, Any] = {
        "dataset": dataset_path.name,
        "dataset_path": str(dataset_path),
        "num_tasks": len(tasks),
        "timestamp": datetime.now(UTC).isoformat(timespec="seconds"),
        "run_id": run_id,
        "classical_method": args.classical_method,
        "config": {
            "reps": args.reps,
            "maxiter": args.maxiter,
            "shots": args.shots,
            "final_shots": args.final_shots,
            "balance_penalty_multiplier": args.balance_penalty,
            "balance_penalty_actual": qaoa_res.get("balance_penalty_actual"),
            "priority_bias": args.priority_bias,
            "optimizer": args.optimizer,
            "restarts": args.restarts,
            "backend": args.backend,
            "seed": args.seed,
        },
        "qaoa": qaoa_res,
        "classical": classical_res,
        "metrics": {
            "qaoa_makespan": qaoa_makespan,
            "min_makespan_in_samples": min_makespan_in_samples,
            "classical_makespan": classical_makespan,
            "relative_gap_pct": relative_gap,
            "best_sample_gap_pct": best_sample_gap,
        },
    }

    output_path = Path(output_override) if output_override else build_output_path(dataset_path, args, run_id)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(json.dumps(payload, indent=2))
    payload["output_path"] = str(output_path)

    summary = payload["metrics"]
    print(
        f"Saved {output_path} | makespans (QAOA={summary['qaoa_makespan']}, classical={summary['classical_makespan']}), gap={summary['relative_gap_pct']}"
    )

    return payload


def run_from_config(config_path: Path, base_args: argparse.Namespace) -> None:
    config = yaml.safe_load(config_path.read_text()) or {}
    experiments = config.get("experiments", [])
    if not experiments:
        raise ValueError(f"No experiments defined in {config_path}")

    output_csv = Path(config.get("output_csv") or (ROOT / "results" / f"{config_path.stem}.csv"))
    num_runs = config.get("num_runs", base_args.num_runs)
    rows = []

    for exp in experiments:
        overrides = vars(base_args).copy()
        overrides.update(exp)
        exp_args = Namespace(**overrides)
        exp_args.num_runs = num_runs
        
        if num_runs > 1:
            # Run multiple independent experiments
            for run_id in range(num_runs):
                payload = run_single_experiment(
                    exp_args,
                    dataset_override=exp.get("dataset"),
                    output_override=exp.get("output"),
                    run_id=run_id,
                )
                metrics = payload["metrics"]
                rows.append(
                    {
                        "dataset": payload["dataset"],
                        "num_tasks": payload["num_tasks"],
                        "run_id": payload.get("run_id"),
                        "reps": exp_args.reps,
                        "shots": exp_args.shots,
                        "final_shots": exp_args.final_shots,
                        "balance_penalty_multiplier": payload["qaoa"].get("balance_penalty_multiplier"),
                        "balance_penalty_actual": payload["qaoa"].get("balance_penalty_actual"),
                        "makespan_penalty_multiplier": exp_args.makespan_penalty,
                        "priority_bias": exp_args.priority_bias,
                        "optimizer": exp_args.optimizer,
                        "restarts": exp_args.restarts,
                        "backend": exp_args.backend,
                        "seed": exp_args.seed,
                        "classical_method": payload.get("classical_method", "brute_force"),
                        "qaoa_makespan": metrics.get("qaoa_makespan"),
                        "min_makespan_in_samples": metrics.get("min_makespan_in_samples"),
                        "classical_makespan": metrics.get("classical_makespan"),
                        "relative_gap_pct": metrics.get("relative_gap_pct"),
                        "best_sample_gap_pct": metrics.get("best_sample_gap_pct"),
                        "output_path": payload["output_path"],
                    }
                )
        else:
            payload = run_single_experiment(
                exp_args,
                dataset_override=exp.get("dataset"),
                output_override=exp.get("output"),
                run_id=0,
            )
            metrics = payload["metrics"]
            rows.append(
                {
                    "dataset": payload["dataset"],
                    "num_tasks": payload["num_tasks"],
                    "run_id": payload.get("run_id"),
                    "reps": exp_args.reps,
                    "shots": exp_args.shots,
                    "final_shots": exp_args.final_shots,
                    "balance_penalty_multiplier": payload["qaoa"].get("balance_penalty_multiplier"),
                    "balance_penalty_actual": payload["qaoa"].get("balance_penalty_actual"),
                    "priority_bias": exp_args.priority_bias,
                    "optimizer": exp_args.optimizer,
                    "restarts": exp_args.restarts,
                    "backend": exp_args.backend,
                    "seed": exp_args.seed,
                    "classical_method": payload.get("classical_method", "brute_force"),
                    "qaoa_makespan": metrics.get("qaoa_makespan"),
                    "min_makespan_in_samples": metrics.get("min_makespan_in_samples"),
                    "classical_makespan": metrics.get("classical_makespan"),
                    "relative_gap_pct": metrics.get("relative_gap_pct"),
                    "best_sample_gap_pct": metrics.get("best_sample_gap_pct"),
                    "output_path": payload["output_path"],
                }
            )

    output_csv.parent.mkdir(parents=True, exist_ok=True)
    with output_csv.open("w", newline="") as fh:
        writer = csv.DictWriter(
            fh,
            fieldnames=[
                "dataset",
                "num_tasks",
                "run_id",
                "reps",
                "shots",
                "final_shots",
                "balance_penalty_multiplier",
                "balance_penalty_actual",
                "makespan_penalty_multiplier",
                "priority_bias",
                "optimizer",
                "restarts",
                "backend",
                "seed",
                "classical_method",
                "qaoa_makespan",
                "min_makespan_in_samples",
                "classical_makespan",
                "relative_gap_pct",
                "best_sample_gap_pct",
                "output_path",
            ],
        )
        writer.writeheader()
        writer.writerows(rows)

    print(f"Batch summary saved to {output_csv}")


def main() -> None:
    args = parse_args()
    if args.config:
        run_from_config(args.config, args)
    else:
        # Support multiple independent runs for statistical analysis
        if args.num_runs > 1:
            print(f"Running {args.num_runs} independent experiments...")
            results = []
            for run_id in range(args.num_runs):
                print(f"\nRun {run_id + 1}/{args.num_runs}")
                result = run_single_experiment(args, run_id=run_id)
                results.append(result)
            print(f"\nCompleted {args.num_runs} runs. Individual results saved.")
        else:
            run_single_experiment(args, run_id=0)


if __name__ == "__main__":
    main()
